# Notes from the Deep Learning for Microscopy Image Analysis Course
## Day 1
# Virginie Uhlmann (EMBL-EBI) - Deep Learning for bioimage analysis
From 2015 onwards, the number of papers with "Deep learning" in their title has spiked dramatically. One of the most used models for image analysis is U-Net, a convolutional network for biomedical image segmentation, solving this problem with a specific architecture. 
Attempts at classifying images can be split into different tasks such as restoration, partitioning (moving from images to objects within them) and object-based quantification. 
Three main image restoration approaches involve _Deblurring_, so inverting the convolution effects resulting from image acquisition, _Denoising_ and _Image Formation_.
For deblurring, before deep learning there were lots of image theory-based tools manipulating PSFs. On the other hand, restoration using deep learning starts from the data itself. For denoising (additive) there are many algorithms (Noise2Something categories). Another tool specialized for single-molecule localization is DECODE (relying on the U-Net architecture again).
In the context of Image partitioning we can have object recognition, semantic segmentation (boundaries) and instance segmentation (separate categories). Object detection (Waithe _et al._, 2020) works very well also for microscopy images as well as it does for natural objects. Adaptations to bounding-boxes are for example StarDist, where we try to adapt the box into a convex polygon able to capture an object's rough shape (a similar approach is taken by SplineDist). 
Some benchmarking datasets: Caicedo _et al._ 2019 (kaggle data science bowl) and LIVECell. More _generalist_ methods include NucleAIzer and CellPose (where nuclei are predicted on new images from diverse datasets in training).
One other very interesting characteristic about DL is the learning of low-dimensional representations of the data, which may or may not entail biological differences unknown to the model, underscoring the generalization capability.
Another quirky aspect is related to style-transfers, for instance training a network to generate synthetic data from one modality to another, for example going from a binary mask to a "constructed" fluorescent image or a completely synthetic H&E staining (Gatys _et al._, 2016).
Another application is related to the ability of DL to integrate image data and omics (Zinchenko _et al._, 2022 BiorXiv).

# Florian Jug (HT) - The Neural Networks Boot Camp
ML is thought of being a subset of AI which entails the performing of specific tasks by learning trends in the data, and DL is a subset of ML using neural networks. Contrasting programming wih machine learning makes us appreciate how ML is based on data and correct answers to extract rules happening in the system and use these rules to predict outcomes.
The basic unit of a neural network (a "neuron") is called a _perceptron_. Inputs with associated weigths are passed to one non-linear activation function (sigmoid, tanh or ReLU - usually a network has a single activation function). The biggest question while building a neuron and a network is the attribution of input-specific weights. Once a prediction is made, we can compare that with ground truth and _backpropagate_ errors through a _loss function_ (more in detail later) to re-enforce specific edges to match ground truth better. The derivative of the loss function trhough back propagation commands how much change is needed to weights.
What a neural network is essentially learning is a decision boundary which can be multi-dimensional based on the complexity of the network and the activation function (visualize it [here](https://playground.tensorflow.org)).

<img src="./pics/nnet_scheme.png" alt="drawing" width="150" class="center">

So, neural networks can get extraordinarily complicated and heavy, so a solution can be _convolution_ which allows us to get weigthed sums for each input using a convolutional filter (kernel).Another strategy for convoution is _max pooling_ where you take the maximum of a set of imputs and shrink them to that specific value. So in order to make a model happen we can use a mix of down-sampling and convolution to achieve this goal.
So with this in mind we can re-visit the U-Net architecture which additionally adds "skip connections" which while you scale the image up from the convoluted layers add information from convoluted layers to the devoncolved ones (grey arrows). Skip connections are there because while scaling up you are forcing very sharp gradients, so to recover information useful for reconstruction we apply information coming from the initially convolved layers. The reasons for its success then are the crazy amount of convolution, the lack of complete connections and the addition of ski connections.

<img src="./pics/unet_scheme.png" alt="drawing" width="150" class="center">

When running the network, not a single input is passed, but inputs are passed in batches of size _k_ while backpropagation takes place at once after all of _k_ has been passed. The first layer of the network instead takes the name of _patch size_. A perk of fully convolutional networks is tha they scale very easily with different patch sizes, where we do not need the same patch size between training and prediction, and this does not apply to fully connected layers, where training and test have to have the same exact size.
So one feed-forwarding of inputs and ground truths for each batch (and its backprop) consist of a single step (or one **epoch**), each epoch happens when a model has seen the whole of the training data. After each epoch we can then ask how good did the model work based on the loss (accuracy). 
